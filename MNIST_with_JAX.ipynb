{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST with JAX.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXFYICyuWbb5"
      },
      "source": [
        "import jax\r\n",
        "from jax import nn, lax, random, vmap, jit, value_and_grad\r\n",
        "from jax.nn import initializers\r\n",
        "from jax.experimental import optimizers\r\n",
        "from jax import numpy as jnp\r\n",
        "from jax import random\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "from tensorflow.keras.datasets import mnist\r\n",
        "from tensorflow.data import Dataset\r\n",
        "import sys\r\n",
        "from tensorflow.keras.utils import Progbar"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M36URSp0WkFm"
      },
      "source": [
        "#Here we are loading the dataset from keras\r\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\n",
        "#converting the values into float32\r\n",
        "x_train = x_train.astype('float32')\r\n",
        "#adding extra dimension because the dataset by default is greyscale so the num of channels is one,\r\n",
        "#it looks like this (60000, 28, 28) as you can see there is no channel dimension\r\n",
        "#so we add extra dimension in the last axis (-1) to look like this (60000, 28, 28, 1)\r\n",
        "x_train = jnp.expand_dims(x_train, -1)\r\n",
        "\r\n",
        "#do the same for the test dataset\r\n",
        "x_test = x_test.astype('float32')\r\n",
        "x_test = jnp.expand_dims(x_test, -1)\r\n",
        "\r\n",
        "#specify the batch size\r\n",
        "batch_size = 64\r\n",
        "\r\n",
        "#add the dataset into TF Dataset\r\n",
        "dataset = Dataset.from_tensor_slices((x_train, y_train))\r\n",
        "#divide the dataset into batches and drop any remainders\r\n",
        "train_dataset = dataset.batch(batch_size, drop_remainder=True)\r\n",
        "test_dataset = Dataset.from_tensor_slices((x_test, y_test))\r\n",
        "test_dataset = test_dataset.batch(batch_size, drop_remainder=True)\r\n",
        "\r\n",
        "#get the number of batches\r\n",
        "num_batches = x_train.shape[0] // batch_size"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y54VGRngWpxj"
      },
      "source": [
        "def get_weights(key=random.PRNGKey(100)):\r\n",
        "    'This function returns weights of the convnet'\r\n",
        "    \r\n",
        "    #here we initialize the first layer of the convnet\r\n",
        "    #the shape which is (3,3,1,64) == (H, W, I, O)\r\n",
        "    #where H = height of the filter, W = width of the filter, I is the number of channels of the input image\r\n",
        "    #in mnist we have only one channel because images are grey scale, and finally O = output filters we want\r\n",
        "    #in the first layer we want 64 filter\r\n",
        "    conv1_weights = initializers.glorot_uniform()(shape=(3,3,1,64), key=key)\r\n",
        "    conv1_bias = initializers.zeros(shape=(1,1,1,64), key=key)\r\n",
        "    #here we have input channels = 64 which are the output filters of the first layer\r\n",
        "    #so we need them as an input to the next layer\r\n",
        "    #and we have 32 filters\r\n",
        "    conv2_weights = initializers.glorot_uniform()(shape=(3,3,64,32), key=key)\r\n",
        "    conv2_bias = initializers.zeros(shape=(1,1,1,32), key=key)\r\n",
        "    #same as above\r\n",
        "    conv3_weights = initializers.glorot_uniform()(shape=(3,3,32,16), key=key)\r\n",
        "    conv3_bias = initializers.zeros(shape=(1,1,1,16), key=key)\r\n",
        "    #here we start the dense layers with their biases, the first layer is 22*22*16 which is the product\r\n",
        "    #of the output dimensions of the above conv layer (22,22,16)\r\n",
        "    dense1_weights = initializers.glorot_uniform()(shape=(22*22*16, 512), key=key)\r\n",
        "    #dense bias with shape 128\r\n",
        "    dense1_bias = initializers.zeros(shape=(512,), key=key)\r\n",
        "    \r\n",
        "    #same as above\r\n",
        "    dense2_weights = initializers.glorot_uniform()(shape=(512, 64), key=key)\r\n",
        "    dense2_bias = initializers.zeros(shape=(64,), key=key)\r\n",
        "    dense3_weights = initializers.glorot_uniform()(shape=(64, 10), key=key)\r\n",
        "    dense3_bias = initializers.zeros(shape=(10,), key=key)\r\n",
        "    \r\n",
        "    return [(conv1_weights, conv1_bias), (conv2_weights, conv2_bias), (conv3_weights, conv3_bias), (dense1_weights, dense1_bias), (dense2_weights, dense2_bias), (dense3_weights, dense3_bias)]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LX39z09hWsEr"
      },
      "source": [
        "params = get_weights()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcuRsg2kWurq"
      },
      "source": [
        "def forward_conv(params, inputs):\r\n",
        "    #unpack the params\r\n",
        "    kernel, bias = params\r\n",
        "    #add extra dim to the inputs because we need the batch axis to do the conv operation\r\n",
        "    #so we add batch axis\r\n",
        "    inputs = jnp.expand_dims(inputs, 0)\r\n",
        "    #Here we define the dimenstion specs\r\n",
        "    #we pass the image shape, kernel shape and the dimension specs\r\n",
        "    \r\n",
        "    #the first dimension spec is the how the image dimensions should look like, here I specified the image should be represented with the order\r\n",
        "    #of the following dimensions where N= Number of examples per batch, H=Height of image, W=Width of the image, C=number of channels in the image\r\n",
        "    \r\n",
        "    #the second dimension spec is the kernel dimensions spec which is H=Height of the filter (the kernel), W=Width of the filter, I=number of channel of the input image\r\n",
        "    #in out example we only have 1 channel because MNIST is greyscale, O=number of filters\r\n",
        "    \r\n",
        "    #the final dimension spec is output spec which describes how the output dimensions should be represented\r\n",
        "    dn = lax.conv_dimension_numbers(inputs.shape, kernel.shape, ('NHWC', 'HWIO', 'NHWC'))\r\n",
        "    #then we pass the inputs, the kernel, strides, padding and the specs we defined above\r\n",
        "    #you can think of the dimension number as if you telling the conv layer how the inputs dimensions look like and how the kernel shape looks like\r\n",
        "    #and how the output dimensions should look like\r\n",
        "    out = lax.conv_general_dilated(inputs, kernel, window_strides=(1,1), dimension_numbers=dn, padding='valid')\r\n",
        "    out = jnp.add(out, bias)\r\n",
        "    out = nn.relu(out)\r\n",
        "    return jnp.squeeze(out)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ju1y_24mWv86"
      },
      "source": [
        "#first vmap is very convenient for batch operations, vmap takes a batch and process it all at once\r\n",
        "#it's equivalent to np.stack([x for x in x_train[i, :]]) as if we loop over every example and pass it to the function\r\n",
        "#in_axes means what axes to use from the parameters that we pass to the function\r\n",
        "#in the forward_conv() we pass conv layers params and the inputs.\r\n",
        "#we don't need to use any axes from the params we need it as it is,\r\n",
        "#but in the inputs we need to loop over the batch axes which is the first axes so we pass to the in_axes=(None, 0)\r\n",
        "#which basically means don't do any thing the first parameter but in the second parameter loop over the first axis\r\n",
        "\r\n",
        "#JIT = Just-in-time compilation, which makes the code runs faster\r\n",
        "#check this video for more information https://www.youtube.com/watch?v=svJerixawV0\r\n",
        "\r\n",
        "batch_conv = jit(vmap(forward_conv, in_axes=(None, 0), out_axes=0))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktcg8sI7WxDq"
      },
      "source": [
        "def forward_dense(params, inputs):\r\n",
        "    #unpack the params\r\n",
        "    weights, bias = params\r\n",
        "    out = jnp.dot(inputs, weights) + bias\r\n",
        "    return out"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VC0wT2fgWyh6"
      },
      "source": [
        "#same as the batch_conv() function\r\n",
        "batch_dense = jit(vmap(forward_dense, in_axes=(None, 0), out_axes=0))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJxZzmJSWzwi"
      },
      "source": [
        "def forward_pass(params, inputs):\r\n",
        "    #add batch dimension\r\n",
        "    out = jnp.expand_dims(inputs, 0)\r\n",
        "    #loop over the first conv layers to pass them to forward_conv function\r\n",
        "    for param in params[:3]:\r\n",
        "        out = batch_conv(param, out)\r\n",
        "    #flatten layer\r\n",
        "    out = out.reshape((out.shape[0], -1))\r\n",
        "    \r\n",
        "    #loop over the dense layers except the last one\r\n",
        "    #because we need the last one to have different activation function\r\n",
        "    dense_params = params[3:-1]\r\n",
        "    for param in dense_params:\r\n",
        "        out = forward_dense(param, out)\r\n",
        "        out = nn.relu(out)\r\n",
        "    out = forward_dense(params[-1], out)\\\r\n",
        "    #the last activation function will be softmax\r\n",
        "    return nn.softmax(jnp.squeeze(out))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLGv-lGFW0vS"
      },
      "source": [
        "forward_batch = jit(vmap(forward_pass, in_axes=(None, 0), out_axes=0))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuIEBiOjW2da"
      },
      "source": [
        "def to_categorical(y, num_classes):\r\n",
        "    return (y[:, None] == np.arange(num_classes))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Klz6xFFPW3iq"
      },
      "source": [
        "def to_numbers(y):\r\n",
        "    #changes from categorical data back into normal data\r\n",
        "    #example [0,0,1,0,0,0,0,0,0] => 3\r\n",
        "    return jnp.argmax(y, axis=-1)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0ZPVhtZW4qy"
      },
      "source": [
        "#check this link if you are interested in more information in the negative log likelihood loss\r\n",
        "#https://ljvmiranda921.github.io/notebook/2017/08/13/softmax-and-the-negative-log-likelihood/\r\n",
        "def NegativeLogLikelihood(params, inputs, targets):\r\n",
        "    preds = forward_batch(params, inputs)\r\n",
        "    return jnp.mean(-jnp.log(preds[targets]))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3kOE67mW53D"
      },
      "source": [
        "#Setting up the optimizer\r\n",
        "learning_rate = 0.0001\r\n",
        "#optimizers in JAX returns 3 functions\r\n",
        "#the first one is the initialization function\r\n",
        "#the second one is the update function\r\n",
        "#the third one used to get the new params from the optimizer\r\n",
        "init_fn, update_fn, get_params = optimizers.adam(learning_rate)\r\n",
        "#set up the optimizer state by initializing the optimizer using the initialization function\r\n",
        "#and passing the current model params to it\r\n",
        "optimizer_state = init_fn(params)\r\n",
        "\r\n",
        "#defining number of epochs and number of classes\r\n",
        "num_epochs = 2\r\n",
        "num_classes = 10"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEc2g_PBW65q"
      },
      "source": [
        "def step(params, x, y, optimizer_state):\r\n",
        "    #here we update the params\r\n",
        "    #first we get the gradients using the loss with respect to the current params\r\n",
        "    #value_and_grad function do 2 things, first it evaluates the function normally\r\n",
        "    #then it gets the gradients of the function\r\n",
        "    #we need to pass the model params to the loss because we need JAX to keep track of what happens to the params\r\n",
        "    #to get the gradients\r\n",
        "    value, grads = value_and_grad(NegativeLogLikelihood)(params, x, y)\r\n",
        "    #then we pass the grads and the current optimizer state to return new optimizer state\r\n",
        "    optimizer_state = update_fn(0, grads, optimizer_state)\r\n",
        "    #finally we get the params from the optimizer state and return it with the optimizer state and the loss value\r\n",
        "    return get_params(optimizer_state), optimizer_state, value"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4IriHNUcDV5"
      },
      "source": [
        "#here I used progress bar from tensorflow to show the training progress\r\n",
        "#also we add the metrics we want to track\r\n",
        "prgbar = Progbar(num_batches, stateful_metrics=['loss', 'Remaining Epochs']) "
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2xOAm_DW9ey",
        "outputId": "c09c1b75-70ff-4047-f7c6-83ca0a0dd721"
      },
      "source": [
        "#start the training by getting the params from the current optimizer state\r\n",
        "params = get_params(optimizer_state)\r\n",
        "#list to store the losses\r\n",
        "losses = []\r\n",
        "for epoch in range(num_epochs):\r\n",
        "    #sums up the epoch loss then resets to zero\r\n",
        "    epoch_loss = 0\r\n",
        "    #keeps track of the processed batches\r\n",
        "    finished_batches = 0\r\n",
        "    for x, y in train_dataset:\r\n",
        "        #convert the tensors from Tensorflow Eager tensors into numpy tensors\r\n",
        "        x = x.numpy()\r\n",
        "        y = y.numpy()\r\n",
        "        #convert targets into categorical\r\n",
        "        y_cat = to_categorical(y, num_classes)\r\n",
        "        #updates the params by passing the current params, x, y and current optimizer state\r\n",
        "        params, optimizer_state, loss_result = step(params, x, y_cat, optimizer_state)\r\n",
        "        #updates the epoch loss\r\n",
        "        epoch_loss += loss_result\r\n",
        "        #update the finished batches\r\n",
        "        finished_batches += 1\r\n",
        "        #Here we update the progress bar after every update\r\n",
        "        #we add the loss name which was specified when we defined the progress bar\r\n",
        "        #and the value for that metric\r\n",
        "        prgbar.update(finished_batches, values=[('loss', loss_result), ('Remaining Epochs', num_epochs-epoch)])\r\n",
        "    losses.append(epoch_loss)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "937/937 [==============================] - 86s 79ms/step - loss: 0.2978 - Remaining Epochs: 2.0000\n",
            "937/937 [==============================] - 159s 78ms/step - loss: 0.2523 - Remaining Epochs: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvrcpPaXddZw"
      },
      "source": [
        "#load sample from the test dataset\r\n",
        "sample_test_x, sample_test_y = next(iter(test_dataset.take(1)))\r\n",
        "#convert the tensors from tensorflow tensors into numpy\r\n",
        "sample_test_x = sample_test_x.numpy()\r\n",
        "sample_test_y = sample_test_y.numpy()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3FZNd1CW-7y"
      },
      "source": [
        "preds = forward_batch(params, sample_test_x)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gE-BM3zbbdfL"
      },
      "source": [
        "preds_as_nums = to_numbers(preds)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_zIONyqfRw4"
      },
      "source": [
        "accuracy = jnp.sum(preds_as_nums == sample_test_y) / sample_test_y.shape[0]"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C98RNOfRfS_0",
        "outputId": "c562bfe3-b411-42bf-f7b8-67ff88d53122"
      },
      "source": [
        "print(f'Batch 1 Accuracy: {accuracy}')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batch 1 Accuracy: 0.984375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFPg3zgZfWMd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}